# Scenario 4: Real-Time Voice Conversation with AI

A full-stack real-time voice conversation app orchestrated by **.NET Aspire**. Speak into your microphone, get AI responses spoken back â€” all in the browser.

**Pattern:** ğŸ™ï¸ Speak â†’ STT (Parakeet) â†’ AI Brain (OpenAI) â†’ TTS (VibeVoice) â†’ ğŸ”Š Hear response

## Architecture

```
Browser (Blazor)
  â†• WebSocket (audio + JSON messages)
Aspire AppHost
  â”œâ”€â”€ conversation-backend (Python FastAPI)
  â”‚   â”œâ”€â”€ STT: NVIDIA Parakeet / faster-whisper
  â”‚   â”œâ”€â”€ AI: OpenAI gpt-4o-mini
  â”‚   â””â”€â”€ TTS: VibeVoice-Realtime-0.5B
  â””â”€â”€ frontend (Blazor Server)
      â”œâ”€â”€ Push-to-talk microphone capture
      â”œâ”€â”€ Audio playback via Web Audio API
      â””â”€â”€ Chat bubble conversation UI
```

## Prerequisites

| Requirement | Details |
|---|---|
| .NET 10 SDK | [Download](https://dotnet.microsoft.com/download/dotnet/10.0) |
| Aspire workload | `dotnet workload install aspire` |
| Python 3.11+ | [python.org](https://python.org) |
| OpenAI API key | Set as `OPENAI_API_KEY` environment variable |
| GPU (optional) | CUDA 12.1+ recommended for STT + TTS models |

## Quick Start

1. **Install Python dependencies:**
   ```bash
   cd src/scenario-04-meai/backend
   pip install -r requirements.txt
   ```

2. **Set your OpenAI API key:**
   ```bash
   # Windows PowerShell
   $env:OPENAI_API_KEY = "sk-..."

   # Linux / macOS
   export OPENAI_API_KEY="sk-..."
   ```

3. **Run with Aspire:**
   ```bash
   cd src/scenario-04-meai/VoiceLabs.ConversationHost
   dotnet run
   ```

4. Open the Aspire dashboard â†’ click the **frontend** endpoint â†’ start talking!

## How It Works

| Step | Component | What Happens |
|------|-----------|-------------|
| 1 | Frontend | User holds push-to-talk button, mic captures 16kHz PCM audio |
| 2 | WebSocket | Audio chunks sent as binary frames to backend |
| 3 | Backend STT | Parakeet (or faster-whisper) transcribes audio to text |
| 4 | Backend AI | OpenAI gpt-4o-mini generates a conversational response |
| 5 | Backend TTS | VibeVoice synthesizes response as 24kHz WAV audio |
| 6 | WebSocket | Audio sent back as binary frames |
| 7 | Frontend | Web Audio API plays the response, chat bubbles update |

## WebSocket Protocol

The frontend and backend communicate over WebSocket at `/ws/conversation`:

| Direction | Type | Content |
|-----------|------|---------|
| Client â†’ Server | Binary | PCM audio chunks (16kHz, 16-bit, mono) |
| Client â†’ Server | Text | `{"type": "end_of_speech"}` signals recording complete |
| Server â†’ Client | Text | `{"type": "transcript", "text": "..."}` |
| Server â†’ Client | Text | `{"type": "response", "text": "..."}` |
| Server â†’ Client | Binary | WAV audio chunks |
| Server â†’ Client | Text | `{"type": "audio_complete"}` |

## Configuration

### Change the AI Model

Edit `backend/app/services/chat_service.py`:
```python
self.model = "gpt-4o"  # or any OpenAI-compatible model
```

### Change the Voice

Select a voice in the frontend dropdown, or set the default in the backend.
Available voices: Carter, Davis, Emma, Frank, Grace, Mike.

### STT Model

The backend tries NVIDIA Parakeet first, then falls back to faster-whisper. Configure in `backend/app/services/stt_service.py`.

## Project Structure

```
scenario-04-meai/
â”œâ”€â”€ VoiceLabs.slnx                    # Solution file
â”œâ”€â”€ VoiceLabs.ConversationHost/       # Aspire AppHost
â”‚   â””â”€â”€ AppHost.cs                    # Orchestrates backend + frontend
â”œâ”€â”€ backend/                          # Python FastAPI conversation service
â”‚   â”œâ”€â”€ main.py                       # FastAPI app + WebSocket endpoint
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”‚   â”œâ”€â”€ stt_service.py        # Speech-to-text (Parakeet)
â”‚   â”‚   â”‚   â”œâ”€â”€ chat_service.py       # AI brain (OpenAI)
â”‚   â”‚   â”‚   â””â”€â”€ tts_service.py        # Text-to-speech (VibeVoice)
â”‚   â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”‚   â”œâ”€â”€ routes.py             # REST endpoints
â”‚   â”‚   â”‚   â””â”€â”€ websocket_handler.py  # WebSocket conversation loop
â”‚   â”‚   â””â”€â”€ models/
â”‚   â”‚       â””â”€â”€ schemas.py            # Pydantic models
â”‚   â””â”€â”€ requirements.txt
â”œâ”€â”€ VoiceLabs.ConversationWeb/        # Blazor frontend
â”‚   â”œâ”€â”€ Program.cs                    # Aspire service defaults + HttpClient
â”‚   â”œâ”€â”€ Components/Pages/Home.razor   # Conversation UI
â”‚   â””â”€â”€ wwwroot/js/audio.js           # Mic capture + audio playback
â”œâ”€â”€ VoiceLabs.ServiceDefaults/        # Aspire shared config
â”œâ”€â”€ Program.cs.bak                    # Old console app (reference)
â””â”€â”€ README.md                         # You are here
```
